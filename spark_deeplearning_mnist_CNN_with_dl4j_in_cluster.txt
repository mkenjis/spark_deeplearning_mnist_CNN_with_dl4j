
//import org.nd4j.linalg.activations.Activation
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator
//import org.nd4j.evaluation.classification.Evaluation
import org.deeplearning4j.nn.conf.MultiLayerConfiguration
import org.deeplearning4j.nn.conf.NeuralNetConfiguration
//import org.nd4j.linalg.learning.config.Nesterovs
import org.deeplearning4j.nn.conf.Updater.ADAGRAD
import org.deeplearning4j.nn.conf.layers.ConvolutionLayer
import org.deeplearning4j.nn.conf.layers.SubsamplingLayer
import org.deeplearning4j.nn.conf.layers.DenseLayer
import org.deeplearning4j.nn.conf.layers.OutputLayer
import org.deeplearning4j.nn.conf.layers.setup.ConvolutionLayerSetup
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.weights.WeightInit
//import org.deeplearning4j.optimize.listeners.ScoreIterationListener
//import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.dataset.DataSet
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction
//import org.slf4j.Logger
//import org.slf4j.LoggerFactory
import org.deeplearning4j.nn.api.{Layer, OptimizationAlgorithm}
import org.apache.spark.storage.StorageLevel

//val conf = new SparkConf().setMaster("spark://master:7077").setAppName("MNIST_CNN")
//                          .set(SparkDl4jMultiLayer.AVERAGE_EACH_ITERATION, String.valueOf(true))
//val sc = new SparkContext(conf)

val nCores = 2
val nChannels = 1
val outputNum = 10
val numSamples = 5000  // 60000
val nTrain = 4500     // 50000
val nTest = 500       // 10000
val batchSize = 64
val iterations = 1
val seed = 123

val mnistIter = new MnistDataSetIterator(1,numSamples, true)

import scala.collection.mutable.ListBuffer
val allData = new ListBuffer[DataSet]()

while(mnistIter.hasNext) allData.+=(mnistIter.next) 

//new Random(12345).shuffle(allData)
val iter = allData.iterator
var c =0
val train = new ListBuffer[DataSet]()
val test = new ListBuffer[DataSet]()
while(iter.hasNext) {
 if(c <= nTrain) {
    train.+=(iter.next)
    c +=1
 }
    else test.+=(iter.next)
}

val sparkDataTrain = sc.parallelize(train)
sparkDataTrain.persist(StorageLevel.MEMORY_ONLY)

println("Building model ....")
val builder = new NeuralNetConfiguration.Builder().
  seed(seed).
  iterations(iterations).
  regularization(true).
  l2(0.0005).
  learningRate(0.01).
  optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).
  updater(org.deeplearning4j.nn.conf.Updater.ADAGRAD).
  list(6).
  layer(0, new ConvolutionLayer.Builder(5, 5).
    nIn(nChannels).
    stride(2, 2).
    nOut(20).
    weightInit(WeightInit.XAVIER).
    activation("relu").
    build()).
  layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, Array(2, 2)).
    build()).
  layer(2, new ConvolutionLayer.Builder(5, 5).
    nIn(20).
    nOut(50).
    stride(2,2).
    weightInit(WeightInit.XAVIER).
    activation("relu").
    build()).
  layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, Array(2, 2)).
    build()).
  layer(4, new DenseLayer.Builder().
    activation("relu").
    weightInit(WeightInit.XAVIER).
    nOut(200).build()).
  layer(5, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD).
    nOut(outputNum).
    weightInit(WeightInit.XAVIER).
    activation("softmax").
    build()).
  backprop(true).pretrain(false)

new ConvolutionLayerSetup(builder,28,28,1)

val multiLayerConf:MultiLayerConfiguration = builder.build()

val net:MultiLayerNetwork = new MultiLayerNetwork(multiLayerConf)
net.init()
net.setUpdater(null)

import org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer
val sparkNetwork = new SparkDl4jMultiLayer(sc, net)

import org.deeplearning4j.eval.Evaluation
val nEpochs = 5
(0 until nEpochs).foreach{i => val network = sparkNetwork.fitDataSet(sparkDataTrain, nCores*batchSize)
 //Evaluate the model
 val eval = new Evaluation()
 for(ds <- test)
 {
    val output = network.output(ds.getFeatureMatrix)
    eval.eval(ds.getLabels, output)
 }
 println("Statistics..."+eval.stats())
}